nowcast <- function(data, model, conf.level = 0.90) {
  #
  # Nowcasting
  #
  # Description
  # Perform nowcast on data using constraind P-spline smoothing
  #
  # Arguments
  # data        Dataframe with data generated by dataSetup
  # model       List with model setup generated by modelSetup
  # conf.level  Confidence level of the prediction interval. Default 90 %
  #
  # Value
  # List with:
  # nowcast    Dataframe with nowcast statistics (med, lwr, upr) by date 1:T
  # F.nowcast  List of length emprical predictive distribution functions by date 1:T
  # f.delay    Dataframe with delay distribution (PMF) by date 1:T and delay 0:D
  
  #
  # Initial stuff
  #
  
  # Filter on records with Est == 1
  data <- data %>% filter(Est == 1)
  
  # Extract data
  n <- data$Cases
  r <- 2 - as.numeric(data$Reported)
  
  # Extract dimensions
  T  <- data$t %>% unique %>% length
  D1 <- data$d %>% unique %>% length
  
  # Extract matrices
  B  <- model$matrices$B
  X  <- model$matrices$X
  BX <- model$matrices$BX
  
  # Get number of coefficients
  Ks <- ncol(B)
  Kw <- ncol(X)
  
  #
  # Estimate parameters
  #
  
  # Initial alpha, beta and theta
  alpha.beta0 <- coef(lm(log(data$Cases + 0.1) ~ as.matrix(BX) - 1))
  theta0 <- 2
  
  # Estimate parameters
  opt <- greedyGridSearch(
    # Function to be optimized
    fn = function(lambda, ...) estimateAlphaBetaTheta(lambda = lambda, ...)$bic,
    # Set lower and upper boundaries for lambda's
    start = c(10, 1e-4),
    lower = c(10, 1e-4)/100,
    upper = c(10, 1e-4)*100,
    # Set grid size
    n.grid = 21,
    # Optimize lambda's on log-scale
    log = c(TRUE, TRUE),
    # Pass data, model and starting values to fn
    data = data,
    model = model,
    alpha.beta = alpha.beta0,
    theta = theta0)
  
  # Get final parameter estimates after optimization of lambda's
  fit <- estimateAlphaBetaTheta(
    lambda = opt$par,
    data = data,
    model = model,
    alpha.beta = alpha.beta0,
    theta = theta0)
  alpha.beta     <- fit$alpha.beta
  alpha.beta.cov <- fit$alpha.beta.cov
  theta          <- fit$theta
  alpha <- alpha.beta[1:Ks]
  beta  <- alpha.beta[(Ks + 1):(Ks + Kw)]
  
  #
  # Nowcast
  #
  
  # 1. Generate n.samples of the parameter estimates
  #    alpha.beta.sim is a Ks + Kw x n.samples matrix
  n.samples <- 1000
  alpha.beta.sim <- alpha.beta +
    (alpha.beta.cov %>% chol %>% t)%*%matrix(
      rnorm(n = (Ks + Kw)*n.samples),
      nrow = Ks + Kw,
      ncol = n.samples)
  
  # 2. Generate n.sim realizations for the not-yet-reported eta and mu
  #    eta.sim and mu.sim are sum(!r) x n.samples matrices
  eta.sim <- as.matrix(BX[!r, ] %*% alpha.beta.sim)
  mu.sim  <- exp(eta.sim)
  print("hallo")
  # 3. Generate n.samples for the not-yet-reported cases
  #    The already reported cases n are fixed!
  #    n.sim is an T x D1 x n.samples array
  n.sim <- array(n, dim = c(T, D1, n.samples))
  for (i in 1:n.samples) {
    n.sim[, , i][!r] <- rnbinom(
      n = sum(!r),
      mu = mu.sim[, i],
      size = theta)
  }
  
  # 4. Sum over delays by date (keep margins 1 and 3) = epicurve
  #    N.sim is a T x n.samples matrix
  N.sim <- apply(
    X = n.sim,
    MARGIN = c(1, 3),
    FUN = sum)
  
  # 5. Get empirical cumulative predictive distribution function by date (keep margin 1)
  #    F.N is a list of length T with ECDFs
  F.N <- apply(
    X = N.sim,
    MARGIN = 1,
    FUN = ecdf)
  # Additionally, calculate statistics from F.N
  #original
  #N.stat <- t(sapply(
  #  X = F.N,
  #  FUN = quantile,
  #  probs = 0.5 + conf.level*c(0, -1, 1)/2))
  #colnames(N.stat) <- c("med", "lwr", "upr")
  
  quantiles <- c(
    0.025,0.1, 0.2,0.25, 0.35, 0.5, 0.65,0.75, 0.8,0.9,0.975
  )  # Beispiel: 10%, 25%, 50%, 75%, 90%
  
  
  
  # Berechnung der Quantile (lwr, med, upr) und des Mittelwerts
  N.stat <- t(sapply(
    X = F.N,
    FUN = function(x) {
      # Berechnung der zusätzlichen Quantile
      q_values <- quantile(x, probs = quantiles, na.rm = TRUE)  
      
      # Berechnung von lwr, med, upr anhand von conf.level
      lwr <- quantile(x, probs = 0.5 + conf.level * -1 / 2, na.rm = TRUE)
      med <- quantile(x, probs = 0.5, na.rm = TRUE)  # Der Median bleibt 50%-Quantil
      upr <- quantile(x, probs = 0.5 + conf.level * 1 / 2, na.rm = TRUE)
      #mean_val <- mean(x, na.rm = TRUE)
      
      # Rückgabe als Vektor
      c(q_values, lwr, med, upr)
    }
  ))
  
  # Erstellen der entsprechenden Spaltennamen für die zusätzlichen Quantile und den Mittelwert
  quantile_names <- paste0("q", quantiles * 100)  # z.B. "q10", "q25", "q50", "q75", "q90"
  # Die neuen Namen für lwr, med, upr und mean
  extra_names <- c("lwr", "med", "upr")
  
  # Kombiniere alle Spaltennamen
  colnames(N.stat) <- c(quantile_names, extra_names)
  
  
  
  #
  # Delay distribution (PMF) by date
  #
  
  # Surface is for Monday, but is the same for any other day because of division by row sums
  # f.delay is a T x D1 matrix
  eta.s <- B %*% alpha %>% as.vector %>% matrix(nrow = T, ncol = D1)
  mu.s <- exp(eta.s)
  f.delay <- mu.s/rowSums(mu.s)
  
  #
  # Return output
  #
  
  return(list(
    # Nowcast statistics by date
    nowcast = cbind(
      data.frame(Date = data$Date %>% unique),
      as.data.frame(N.stat)),
    # Nowcast predictive distributions (CDF) by date
    F.nowcast = F.N, 
    # Delay distribution (PMF) by date
    f.delay = cbind(
      data[, c("Date", "Delay", "Reported")],
      data.frame(f.delay = as.vector(f.delay)))))
  
}










nowcast_tryotu <- function(data, model, conf.level = 0.90) {
  #
  # Nowcasting
  #
  # Description
  # Perform nowcast on data using constraind P-spline smoothing
  #
  # Arguments
  # data        Dataframe with data generated by dataSetup
  # model       List with model setup generated by modelSetup
  # conf.level  Confidence level of the prediction interval. Default 90 %
  #
  # Value
  # List with:
  # nowcast    Dataframe with nowcast statistics (med, lwr, upr) by date 1:T
  # F.nowcast  List of length emprical predictive distribution functions by date 1:T
  # f.delay    Dataframe with delay distribution (PMF) by date 1:T and delay 0:D
  
  #
  # Initial stuff
  #
  
  # Filter on records with Est == 1
  data <- data %>% filter(Est == 1)
  print("n und r")
  # Extract data
  n <- data$Cases
  r <- 2 - as.numeric(data$Reported)
  print(n)
  print(r)
  # Extract dimensions
  T  <- data$t %>% unique %>% length
  D1 <- data$d %>% unique %>% length
  
  # Extract matrices
  B  <- model$matrices$B
  X  <- model$matrices$X
  BX <- model$matrices$BX
  
  # Get number of coefficients
  Ks <- ncol(B)
  Kw <- ncol(X)
  
  #
  # Estimate parameters
  #
  
  # Initial alpha, beta and theta
  alpha.beta0 <- coef(lm(log(data$Cases + 0.1) ~ as.matrix(BX) - 1))
  theta0 <- 2
  
  # Estimate parameters
  opt <- greedyGridSearch(
    # Function to be optimized
    fn = function(lambda, ...) estimateAlphaBetaTheta(lambda = lambda, ...)$bic,
    # Set lower and upper boundaries for lambda's
    start = c(10, 1e-4),
    lower = c(10, 1e-4)/100,
    upper = c(10, 1e-4)*100,
    # Set grid size
    n.grid = 21,
    # Optimize lambda's on log-scale
    log = c(TRUE, TRUE),
    # Pass data, model and starting values to fn
    data = data,
    model = model,
    alpha.beta = alpha.beta0,
    theta = theta0)
  #print("problem außen")
  # Get final parameter estimates after optimization of lambda's
  fit <- estimateAlphaBetaTheta(
    lambda = opt$par,
    data = data,
    model = model,
    alpha.beta = alpha.beta0,
    theta = theta0)
  print("problem nach fit")
  alpha.beta     <- fit$alpha.beta
  alpha.beta.cov <- fit$alpha.beta.cov
  theta          <- fit$theta
  alpha <- alpha.beta[1:Ks]
  #beta  <- alpha.beta[(Ks + 1):(Ks + Kw)]
  
  #
  # Nowcast
  #
  
  # 1. Generate n.samples of the parameter estimates
  #    alpha.beta.sim is a Ks + Kw x n.samples matrix
  n.samples <- 1000
  alpha.beta.sim <- alpha.beta +
    (alpha.beta.cov %>% chol %>% t)%*%matrix(
      rnorm(n = (Ks + Kw)*n.samples),
      nrow = Ks + Kw,
      ncol = n.samples)
  
  # 2. Generate n.sim realizations for the not-yet-reported eta and mu
  #    eta.sim and mu.sim are sum(!r) x n.samples matrices
  eta.sim <- as.matrix(BX[!r, ] %*% alpha.beta.sim)
  mu.sim  <- exp(eta.sim)
  
  # 3. Generate n.samples for the not-yet-reported cases
  #    The already reported cases n are fixed!
  #    n.sim is an T x D1 x n.samples array
  n.sim <- array(n, dim = c(T, D1, n.samples))
  for (i in 1:n.samples) {
    n.sim[, , i][!r] <- rnbinom(
      n = sum(!r),
      mu = mu.sim[, i],
      size = theta)
  }
  print("nsim -< onch delay")
  print(n.sim)
  # 4. Sum over delays by date (keep margins 1 and 3) = epicurve
  #    N.sim is a T x n.samples matrix
  N.sim <- apply(
    X = n.sim,
    MARGIN = 1,# c(1, 3),
    FUN = sum)
  print("N.sim")
  print(N.sim)
  N.sim <- matrix(N.sim)
  
  N.sim <- t(N.sim)
  print("N.sim")
  print(N.sim)
  # 5. Get empirical cumulative predictive distribution function by date (keep margin 1)
  #    F.N is a list of length T with ECDFs
  print(dim(N.sim))
  F.N <- apply(
    X = N.sim,
    MARGIN = 2,
    FUN = ecdf)
  print("F.N")
  print(F.N)
  # Additionally, calculate statistics from F.N
  #N.stat <- t(sapply(
  #  X = F.N,
  #  MARGIN = 2,
  #  FUN = quantile,
  #  probs = c(0.1, 0.5, 0.9)))  # Änderung der Quantile
  
  
  N.stat <- sapply(
    X = F.N,
    FUN = function(x) quantile(x, probs = c(0.1, 0.5, 0.9))
  )
  
  # Die Dimension der Matrix so ändern, dass Quantile als Spalten angezeigt werden
  N.stat <- t(N.stat)  # Transponieren, sodass Quantile in Spalten erscheinen
  
  colnames(N.stat) <- c("q10", "q50", "q90")
  #
  # Delay distribution (PMF) by date
  #
  print("N.stat")
  print(N.stat)
  # Surface is for Monday, but is the same for any other day because of division by row sums
  # f.delay is a T x D1 matrix
  eta.s <- B %*% alpha %>% as.vector %>% matrix(nrow = T, ncol = D1)
  mu.s <- exp(eta.s)
  f.delay <- mu.s/rowSums(mu.s)
  
  #
  # Return output
  #
  
  return(list(
    # Nowcast statistics by date
    nowcast = cbind(
      data.frame(Date = data$Date,
                 Delay = data$Delay %>% unique  # Extrahiere die Delay-Wert
      ),
      as.data.frame(N.stat)),
    # Nowcast predictive distributions (CDF) by date
    F.nowcast = F.N, 
    # Delay distribution (PMF) by date
    f.delay = cbind(
      data[, c("Date", "Delay", "Reported")],
      data.frame(f.delay = as.vector(f.delay)))))
  
}
